{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:06.146786Z",
     "start_time": "2020-09-10T00:17:03.988090Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import copy\n",
    "import gc\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as sps\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import (GridSearchCV, RandomizedSearchCV, \n",
    "                                     cross_val_score)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from hyperopt import hp, tpe, STATUS_OK, Trials\n",
    "from hyperopt.fmin import fmin\n",
    "\n",
    "from src.utils.cross_validation import TimeSeriesGroupSplit\n",
    "from src.utils.downcasting import downcast_dtypes\n",
    "\n",
    "sns.set(font_scale=1.2)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:06.183001Z",
     "start_time": "2020-09-10T00:17:06.149342Z"
    }
   },
   "outputs": [],
   "source": [
    "max_text_features = 20\n",
    "random_state = 42\n",
    "\n",
    "MEAN_CONSTANT = 0.3343"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will produce predictions by XGBoost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will load all datasets and prepare them for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:15.416106Z",
     "start_time": "2020-09-10T00:17:06.188127Z"
    }
   },
   "outputs": [],
   "source": [
    "sales_train = pd.read_csv('../data/processed/sales_train.csv')\n",
    "train = pd.read_feather('../data/processed/train.ftr')\n",
    "test = pd.read_feather('../data/processed/test.ftr')\n",
    "\n",
    "items = pd.read_csv('../data/processed/items.csv')\n",
    "tfidf_truncated_svd = pd.read_feather('../data/processed/text/tfidf_truncated-svd.ftr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:20.860287Z",
     "start_time": "2020-09-10T00:17:15.419270Z"
    }
   },
   "outputs": [],
   "source": [
    "train.drop(columns=['index'], inplace=True)\n",
    "test.drop(columns=['index', 'level_0'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding text features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:20.896070Z",
     "start_time": "2020-09-10T00:17:20.863230Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_truncated_svd = tfidf_truncated_svd[tfidf_truncated_svd.columns[:max_text_features]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:20.930151Z",
     "start_time": "2020-09-10T00:17:20.899119Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_truncated_svd['item_id'] = items.item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:40.260792Z",
     "start_time": "2020-09-10T00:17:20.935522Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.merge(\n",
    "    train,\n",
    "    tfidf_truncated_svd,\n",
    "    how='left', on='item_id'\n",
    ")\n",
    "\n",
    "test = pd.merge(\n",
    "    test,\n",
    "    tfidf_truncated_svd,\n",
    "    how='left', on='item_id'\n",
    ")\n",
    "\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to evaluation, target will be clipped between 0 and 20. Let's do it in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:40.425854Z",
     "start_time": "2020-09-10T00:17:40.265591Z"
    }
   },
   "outputs": [],
   "source": [
    "train.target = np.clip(train.target, 0, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process categorical data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:40.479954Z",
     "start_time": "2020-09-10T00:17:40.431017Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_types = ['object', 'bool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:40.523111Z",
     "start_time": "2020-09-10T00:17:40.482986Z"
    }
   },
   "outputs": [],
   "source": [
    "train.dtypes[np.isin(train.dtypes.values, categorical_types)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't touch boolean objects, they are already label encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove columns `item_name`, `shop_name`, because we already have them label encoded as `item_id`, `shop_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:55.335203Z",
     "start_time": "2020-09-10T00:17:40.528528Z"
    }
   },
   "outputs": [],
   "source": [
    "train.drop(columns=['item_name', 'shop_name'], inplace=True)\n",
    "test.drop(columns=['item_name', 'shop_name'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define list with all categorical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:55.375215Z",
     "start_time": "2020-09-10T00:17:55.341286Z"
    }
   },
   "outputs": [],
   "source": [
    "categorical_features = [\n",
    "    'month', 'item_id', 'item_full_category_name', 'item_category_name', \n",
    "    'item_subcategory_name', 'shop_id', 'city'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will label encode only `city`, because in other cases we have already label encoded features or there are values on test, that are not present on train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:17:56.962473Z",
     "start_time": "2020-09-10T00:17:55.377720Z"
    }
   },
   "outputs": [],
   "source": [
    "label_encode_features = ['city']\n",
    "\n",
    "for column in label_encode_features:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    encoded_feature_train = le.fit_transform(train[column])\n",
    "    train[f'{column}_labeled'] = encoded_feature_train\n",
    "    \n",
    "    encoded_feature_test = le.transform(test[column])\n",
    "    test[f'{column}_labeled'] = encoded_feature_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:18:50.764555Z",
     "start_time": "2020-09-10T00:17:56.965354Z"
    }
   },
   "outputs": [],
   "source": [
    "for column in tqdm(categorical_features):\n",
    "    # encode train\n",
    "    cumsum = train.groupby(column).target.cumsum() - train.target\n",
    "    cumcount = train.groupby(column).cumcount()\n",
    "    encoded_feature = cumsum / cumcount\n",
    "    encoded_feature.fillna(MEAN_CONSTANT, inplace=True)\n",
    "    \n",
    "    train[f'{column}_mean_encoded_mean'] = encoded_feature\n",
    "    \n",
    "    # encode test\n",
    "    mean_train = train.groupby(column).target.mean()\n",
    "    test[f'{column}_mean_encoded_mean'] = test[column].map(mean_train).fillna(MEAN_CONSTANT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop all redundant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:18:58.315579Z",
     "start_time": "2020-09-10T00:18:50.774512Z"
    }
   },
   "outputs": [],
   "source": [
    "to_drop = ['item_id', 'item_full_category_name', 'item_category_name', \n",
    "           'item_subcategory_name', 'city']\n",
    "train.drop(columns=to_drop, inplace=True)\n",
    "test.drop(columns=to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:19:02.944280Z",
     "start_time": "2020-09-10T00:18:58.318390Z"
    }
   },
   "outputs": [],
   "source": [
    "train.columns[train.isna().sum() > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we expected there are some problems only with `num_residents`. We can fill it with zero, because it will be border value for this feature, trees can handle it properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:19:04.775820Z",
     "start_time": "2020-09-10T00:19:02.947518Z"
    }
   },
   "outputs": [],
   "source": [
    "train.fillna(0, inplace=True)\n",
    "test.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove target from train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:19:09.243888Z",
     "start_time": "2020-09-10T00:19:04.777802Z"
    }
   },
   "outputs": [],
   "source": [
    "y = train.target\n",
    "train.drop(columns=['target'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of validation split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also delete from train rows that appears only on validation, it will make our train/validation split more consistant with train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:19:15.249560Z",
     "start_time": "2020-09-10T00:19:09.245766Z"
    }
   },
   "outputs": [],
   "source": [
    "X_valid = train[train.date_block_num == 33]\n",
    "X_train = train[train.date_block_num < 33]\n",
    "y_valid = y[train.date_block_num == 33]\n",
    "y_train = y[train.date_block_num < 33]\n",
    "X_test = test\n",
    "    \n",
    "del train, test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will find optimum parameters for a model. Firstly, fix the result before any optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T00:19:15.297050Z",
     "start_time": "2020-09-10T00:19:15.253800Z"
    }
   },
   "outputs": [],
   "source": [
    "ts = TimeSeriesGroupSplit(n_splits=5, max_train_size=int(1.5*10**6))\n",
    "\n",
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.1,\n",
    "    'seed': random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T01:21:35.064524Z",
     "start_time": "2020-09-10T00:19:15.300237Z"
    }
   },
   "outputs": [],
   "source": [
    "default_score = cross_val_score(\n",
    "    XGBRegressor(**xgb_params), \n",
    "    X_train, y_train, groups=X_train.date_block_num,\n",
    "    n_jobs=1, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=0,\n",
    "    cv=ts\n",
    ").mean()\n",
    "print(f'Current score: {-default_score:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: $0.91020$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `n_estimators`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we fix `learning_rate = 0.1` and try to find reasonable num of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T01:21:35.347721Z",
     "start_time": "2020-09-10T01:21:35.074994Z"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_dataset = xgb.DMatrix(X_train[X_train.date_block_num >= 25], \n",
    "#                               label=y_train[X_train.date_block_num >= 25])\n",
    "# X_valid_dataset = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "# xgb.train(xgb_params, X_train_dataset, num_boost_round=500, \n",
    "#           evals=[(X_train_dataset, 'train'), (X_valid_dataset, 'test')], \n",
    "#           early_stopping_rounds=100)\n",
    "\n",
    "# del X_train_dataset, X_valid_dataset\n",
    "# gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T20:38:26.259442Z",
     "start_time": "2020-09-05T20:38:26.227411Z"
    }
   },
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'n_estimators': np.arange(50, 301, 50)\n",
    "# }\n",
    "\n",
    "# gs = GridSearchCV(\n",
    "#     LGBMRegressor(**lgb_params), \n",
    "#     param_grid,          \n",
    "#     n_jobs=1, \n",
    "#     scoring='neg_root_mean_squared_error', \n",
    "#     verbose=10,\n",
    "#     refit=False,\n",
    "#     cv=ts\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T18:06:22.530172Z",
     "start_time": "2020-09-04T17:53:23.814280Z"
    }
   },
   "source": [
    "gs.fit(X_train, y_train, groups=X_train.date_block_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T18:06:22.592080Z",
     "start_time": "2020-09-04T18:06:22.536120Z"
    }
   },
   "source": [
    "gs.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T18:06:33.368544Z",
     "start_time": "2020-09-04T18:06:33.333062Z"
    }
   },
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T20:38:26.302792Z",
     "start_time": "2020-09-05T20:38:26.262519Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params['n_estimators'] = 243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree parameters, subsampling, regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will find optimum values for building tree:\n",
    "* `max_depth`\n",
    "* `min_child_weight`\n",
    "\n",
    "For subsampling:\n",
    "* `subsample`\n",
    "* `colsample_bytree`\n",
    "\n",
    "For regularization:\n",
    "* `gamma`\n",
    "* `alpha`\n",
    "* `lambda`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T21:04:09.446541Z",
     "start_time": "2020-09-05T21:04:09.354984Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'learning_rate': 0.1,\n",
    "    'seed': random_state\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-05T21:24:43.665882Z",
     "start_time": "2020-09-05T21:24:43.627672Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    \"\"\"Function to optimize in hyperopt.\"\"\"\n",
    "    params['n_estimators'] = int(params['n_estimators'])\n",
    "    params['max_depth'] = int(params['max_depth'])\n",
    "    \n",
    "    current_params = copy.copy(xgb_params)\n",
    "    current_params.update(params)    \n",
    "    score = cross_val_score(\n",
    "        XGBRegressor(**current_params), \n",
    "        X_train, y_train, groups=X_train.date_block_num,\n",
    "        n_jobs=1, \n",
    "        scoring='neg_root_mean_squared_error', \n",
    "        verbose=0,\n",
    "        cv=ts\n",
    "    ).mean()\n",
    "    \n",
    "    print(f'RMSE {-score:.5f} params {params}')\n",
    "    \n",
    "    return {\n",
    "        'loss': -score,\n",
    "        'status': STATUS_OK,\n",
    "    }\n",
    "\n",
    "space = {\n",
    "    'n_estimators': hp.quniform('n_estimators', 10, 100, 5),\n",
    "    'max_depth': hp.quniform('max_depth', 3, 12, 1),\n",
    "    'min_child_weight': hp.loguniform('min_child_weight', 0, np.log(1e3)),\n",
    "    'subsample': hp.uniform('subsample', 0, 1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0, 1),\n",
    "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
    "    'alpha': hp.choice('alpha', [0, hp.loguniform('alpha_not_zero', np.log(1e-3), 0)]),\n",
    "    'lambda': hp.choice('lambda', [0, hp.loguniform('lambda_not_zero', np.log(1e-3), 0)])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T04:12:08.018199Z",
     "start_time": "2020-09-05T21:27:49.395061Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(random_state)\n",
    "trials = Trials()\n",
    "best = fmin(\n",
    "    fn=objective, \n",
    "    space=space, \n",
    "    algo=tpe.suggest, \n",
    "    max_evals=120,\n",
    "    trials=trials\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T07:28:30.613761Z",
     "start_time": "2020-09-06T07:28:30.570420Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../models/xgb/hyperopt/trials.pkl', 'wb') as ouf:\n",
    "    pickle.dump(trials, ouf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T07:24:01.109715Z",
     "start_time": "2020-09-06T07:24:00.847012Z"
    }
   },
   "outputs": [],
   "source": [
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T01:21:35.410283Z",
     "start_time": "2020-09-10T01:21:35.361834Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params['max_depth'] = 6\n",
    "xgb_params['min_child_weight'] = 66.773\n",
    "\n",
    "xgb_params['subsample'] = 0.576\n",
    "xgb_params['colsample_bytree'] = 0.898\n",
    "\n",
    "xgb_params['gamma'] = 0.361\n",
    "xgb_params['alpha'] = 0\n",
    "xgb_params['lambda'] = 0\n",
    "\n",
    "xgb_params['n_estimators'] = 243"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:27:04.714640Z",
     "start_time": "2020-09-10T01:21:35.413603Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "current_score = cross_val_score(\n",
    "    XGBRegressor(**xgb_params), \n",
    "    X_train, y_train, groups=X_train.date_block_num,\n",
    "    n_jobs=1, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=0,\n",
    "    cv=ts\n",
    ").mean()\n",
    "print(f'Current score: {-current_score:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: $0.91447$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can half our `learning rate` and double `n_estimators`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:27:05.028731Z",
     "start_time": "2020-09-10T03:27:04.726523Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T03:27:05.075029Z",
     "start_time": "2020-09-10T03:27:05.031888Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params_changed = copy.copy(xgb_params)\n",
    "xgb_params_changed['learning_rate'] /= 2\n",
    "xgb_params_changed['n_estimators'] = int(xgb_params['n_estimators'] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T07:33:40.159756Z",
     "start_time": "2020-09-10T03:27:05.078098Z"
    }
   },
   "outputs": [],
   "source": [
    "score_after_change = cross_val_score(\n",
    "    XGBRegressor(**xgb_params_changed), \n",
    "    X_train, y_train, groups=X_train.date_block_num,\n",
    "    n_jobs=1, \n",
    "    scoring='neg_root_mean_squared_error', \n",
    "    verbose=0,\n",
    "    cv=ts\n",
    ").mean()\n",
    "\n",
    "print(f'Score after changing: {-score_after_change:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: $0.91035$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It helped, but computations was too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T09:30:15.635303Z",
     "start_time": "2020-09-06T09:30:15.524531Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will validate best parameters using haldout. We will use not all train, because of limitation of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T09:58:18.521267Z",
     "start_time": "2020-09-06T09:58:18.474544Z"
    }
   },
   "outputs": [],
   "source": [
    "(X_train.date_block_num >= 25).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T09:58:21.111922Z",
     "start_time": "2020-09-06T09:58:18.663347Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date_block_num = 25\n",
    "indices_train = (X_train.date_block_num >= start_date_block_num)\n",
    "X_train = X_train[indices_train]\n",
    "y_train = y_train[indices_train]\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T10:26:15.668967Z",
     "start_time": "2020-09-06T09:58:21.114249Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(**xgb_params)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T10:26:20.309437Z",
     "start_time": "2020-09-06T10:26:15.678878Z"
    }
   },
   "outputs": [],
   "source": [
    "y_predicted = np.clip(model.predict(X_valid), 0, 20)\n",
    "validation_score = mean_squared_error(y_valid, y_predicted)\n",
    "print(f'Validation score: {validation_score:.5f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score: $0.81906$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at predicted values charasteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T10:39:11.361338Z",
     "start_time": "2020-09-06T10:39:11.291191Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(y_predicted).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T10:41:19.684499Z",
     "start_time": "2020-09-06T10:41:16.596566Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 20))\n",
    "xgb.plot_importance(model, ax=ax);\n",
    "plt.savefig('../reports/figures/xgb/importances.png', facecolor='white', \n",
    "            bbox_inches='tight', pad_inches=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 20))\n",
    "image = plt.imread('../reports/figures/xgb/importances.png')\n",
    "plt.imshow(image, interpolation='spline36')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will train result model and submit prediction. Don't forget to clip values according to [evaluation tab](https://www.kaggle.com/c/competitive-data-science-predict-future-sales/overview/evaluation) (but ay be for tree-based methods it is not necessary)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T07:33:56.939071Z",
     "start_time": "2020-09-10T07:33:40.170059Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train = pd.concat((X_train, X_valid))\n",
    "y_train = pd.concat((y_train, y_valid))\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T07:33:56.969770Z",
     "start_time": "2020-09-10T07:33:56.940988Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T10:41:53.930121Z",
     "start_time": "2020-09-06T10:41:50.409475Z"
    }
   },
   "outputs": [],
   "source": [
    "start_date_block_num = 25\n",
    "indices_train = (X_train.date_block_num >= start_date_block_num)\n",
    "X_train = X_train[indices_train]\n",
    "y_train = y_train[indices_train]\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T13:25:52.403327Z",
     "start_time": "2020-09-06T10:41:53.932479Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(**xgb_params)\n",
    "bags = 5\n",
    "\n",
    "bagged_predictions = np.zeros(X_test.shape[0])\n",
    "for n in tqdm(range(bags)):\n",
    "    model.set_params(random_state=random_state+n)\n",
    "    model.fit(X_train, y_train)\n",
    "    bagged_predictions += np.clip(model.predict(X_test), 0, 20)\n",
    "    gc.collect()\n",
    "    \n",
    "bagged_predictions /= bags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T13:25:52.570935Z",
     "start_time": "2020-09-06T13:25:52.407987Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(bagged_predictions).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-06T13:25:53.358785Z",
     "start_time": "2020-09-06T13:25:52.574169Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../data/raw/sample_submission.csv')\n",
    "submission['item_cnt_month'] = bagged_predictions\n",
    "submission.to_csv('../models/xgb/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-04T07:41:17.285249Z",
     "start_time": "2020-09-04T07:41:08.424971Z"
    }
   },
   "source": [
    "!kaggle competitions submit competitive-data-science-predict-future-sales -f ../models/xgb/submission.csv -m \"XGBoost\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result is $1.01051$. It is pretty far from top positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OOF predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will create out-of-fold predictions for stacking. We will use cheme f), that was given in the course:\n",
    "> In time-series task we usually have a fixed period of time we are asked to predict. Like day, week, month or arbitrary period with duration of T.\n",
    "> 1. Split the train data into chunks of duration T. Select first M chunks.\n",
    "> 2. Fit N diverse models on those M chunks and predict for the chunk M+1. Then fit those models on first M+1 chunks and predict for chunk M+2 and so on, until you hit the end. After that use all train data to fit models and get predictions for test. Now we will have meta-features for the chunks starting from number M+1 as well as meta-features for the test.\n",
    "> 3. Now we can use meta-features from first K chunks [M+1,M+2,..,M+K] to fit level 2 models and validate them on chunk M+K+1. Essentially we are back to step 1. with the lesser amount of chunks and meta-features instead of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T07:34:14.181065Z",
     "start_time": "2020-09-10T07:33:56.973149Z"
    }
   },
   "outputs": [],
   "source": [
    "X_all = pd.concat((X_train, X_test))\n",
    "test_size = X_test.shape[0]\n",
    "del X_train, X_test\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T07:34:14.266615Z",
     "start_time": "2020-09-10T07:34:14.183155Z"
    }
   },
   "outputs": [],
   "source": [
    "num_blocks = X_all.date_block_num.nunique()\n",
    "ts = TimeSeriesGroupSplit(n_splits=num_blocks-1, max_train_size=int(1.5*10**6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use chunks, devided by `date_block_num`. In our case $M = 3$, but we won't use all previous chunks to train and limit it according to `max_train_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T08:26:02.222941Z",
     "start_time": "2020-09-10T07:34:14.269456Z"
    }
   },
   "outputs": [],
   "source": [
    "model = XGBRegressor(**xgb_params)\n",
    "\n",
    "predictions = np.zeros(X_all.shape[0])\n",
    "filled_predictions = np.zeros(X_all.shape[0]).astype(bool)\n",
    "\n",
    "for i, (train_idx, test_idx) in tqdm(\n",
    "    enumerate(ts.split(X_all, groups=X_all.date_block_num)), total=22\n",
    "):\n",
    "    # skip too small training size\n",
    "    if i < 2:\n",
    "        continue\n",
    "    model.fit(X_all.iloc[train_idx], y_train.iloc[train_idx])\n",
    "    current_predictions = model.predict(X_all.iloc[test_idx])\n",
    "    predictions[test_idx] = current_predictions\n",
    "    filled_predictions[test_idx] = True\n",
    "    \n",
    "predictions = predictions[filled_predictions]\n",
    "y_train = y_train.iloc[filled_predictions[:-test_size]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save columns of predictions and clipped predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T08:26:02.228123Z",
     "start_time": "2020-09-10T00:26:00.184Z"
    }
   },
   "outputs": [],
   "source": [
    "OOF_all = pd.DataFrame({'xgb': predictions, \n",
    "                        'xgb_clipped': np.clip(predictions, 0, 20)})\n",
    "OOF_train = OOF_all.iloc[:-test_size].reset_index(drop=True)\n",
    "OOF_train['target'] = y_train.values\n",
    "OOF_test = OOF_all.iloc[-test_size:].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-10T08:26:02.230163Z",
     "start_time": "2020-09-10T00:26:03.398Z"
    }
   },
   "outputs": [],
   "source": [
    "OOF_train.to_csv('../models/oof/xgb/train.csv', index=False)\n",
    "OOF_test.to_csv('../models/oof/xgb/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
